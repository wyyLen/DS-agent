"""
æµ‹è¯•DSAgent Coreä¸MetaGPTçš„é›†æˆæ•ˆæœ

æ­¤è„šæœ¬éªŒè¯ï¼š
1. æ–°é€‚é…å™¨èƒ½å¦æ­£ç¡®åŠ è½½ç°æœ‰çš„ç»éªŒåº“
2. æ–‡æœ¬ç»éªŒæ£€ç´¢æ˜¯å¦ä¸åŸæœ‰ç³»ç»Ÿå…¼å®¹
3. å·¥ä½œæµç»éªŒæ£€ç´¢æ˜¯å¦ä¸åŸæœ‰ç³»ç»Ÿå…¼å®¹
4. æ£€ç´¢ç»“æœæ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
"""

import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from dsagent_core.adapters import MetaGPTAdapter
from metagpt.schema import Plan, Task
from metagpt.const import EXAMPLE_DATA_PATH, EXP_PLAN, WORKFLOW_EXP


def test_text_retrieval():
    """æµ‹è¯•æ–‡æœ¬ç»éªŒæ£€ç´¢"""
    print("=" * 80)
    print("æµ‹è¯• 1: æ–‡æœ¬ç»éªŒæ£€ç´¢")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–é€‚é…å™¨
        adapter = MetaGPTAdapter(text_exp_path=EXP_PLAN)
        print(f"âœ“ æˆåŠŸåˆå§‹åŒ–æ–‡æœ¬æ£€ç´¢å™¨")
        print(f"  ç»éªŒåº“è·¯å¾„: {EXP_PLAN}")
        print(f"  ç»éªŒæ•°é‡: {len(adapter.text_retriever.experiences)}")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "How to analyze correlation between variables?",
            "How to handle missing values in dataset?",
            "How to build a machine learning model for prediction?"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\næŸ¥è¯¢ {i}: {query}")
            start_time = time.time()
            result = adapter.retrieve_text_experiences(query, top_k=3)
            elapsed = time.time() - start_time
            
            print(f"  æ£€ç´¢æ—¶é—´: {elapsed:.3f}ç§’")
            print(f"  æ‰¾åˆ° {len(result.experiences)} æ¡ç›¸å…³ç»éªŒ")
            
            for j, exp in enumerate(result.experiences, 1):
                print(f"\n  ç»éªŒ {j} (å¾—åˆ†: {exp.score:.2f}):")
                content_preview = exp.content[:150].replace('\n', ' ')
                print(f"    å†…å®¹é¢„è§ˆ: {content_preview}...")
                if exp.metadata:
                    print(f"    å…ƒæ•°æ®: {exp.metadata}")
        
        # æµ‹è¯•æ ¼å¼åŒ–è¾“å‡ºï¼ˆç”¨äºLLMæç¤ºï¼‰
        print(f"\n\n--- æ ¼å¼åŒ–è¾“å‡ºæµ‹è¯• ---")
        result = adapter.retrieve_text_experiences(test_queries[0], top_k=2)
        formatted = adapter.format_experiences_for_prompt(result)
        print(f"æ ¼å¼åŒ–åçš„ç»éªŒï¼ˆå‰500å­—ç¬¦ï¼‰:")
        print(formatted[:500])
        
        print(f"\nâœ“ æ–‡æœ¬ç»éªŒæ£€ç´¢æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâœ— æ–‡æœ¬ç»éªŒæ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_workflow_retrieval():
    """æµ‹è¯•å·¥ä½œæµç»éªŒæ£€ç´¢"""
    print("\n\n" + "=" * 80)
    print("æµ‹è¯• 2: å·¥ä½œæµç»éªŒæ£€ç´¢")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–é€‚é…å™¨
        adapter = MetaGPTAdapter(workflow_exp_path=WORKFLOW_EXP)
        print(f"âœ“ æˆåŠŸåˆå§‹åŒ–å·¥ä½œæµæ£€ç´¢å™¨")
        print(f"  ç»éªŒåº“è·¯å¾„: {WORKFLOW_EXP}")
        print(f"  å·¥ä½œæµæ•°é‡: {len(adapter.workflow_retriever.experiences)}")
        
        # åˆ›å»ºæµ‹è¯•ç”¨çš„Plan
        test_plans = [
            # æµ‹è¯•æ¡ˆä¾‹1: ç®€å•çš„æ•°æ®åˆ†ææµç¨‹
            {
                "name": "ç®€å•æ•°æ®åˆ†æ",
                "tasks": [
                    Task(
                        task_id="1",
                        instruction="Load and inspect the dataset",
                        task_type="pda",
                        dependent_task_ids=[]
                    ),
                    Task(
                        task_id="2",
                        instruction="Analyze correlation between features",
                        task_type="correlation analysis",
                        dependent_task_ids=["1"]
                    ),
                    Task(
                        task_id="3",
                        instruction="Visualize the results",
                        task_type="visualization",
                        dependent_task_ids=["2"]
                    )
                ]
            },
            # æµ‹è¯•æ¡ˆä¾‹2: æœºå™¨å­¦ä¹ æµç¨‹
            {
                "name": "æœºå™¨å­¦ä¹ é¢„æµ‹",
                "tasks": [
                    Task(
                        task_id="1",
                        instruction="Load and preprocess data",
                        task_type="pda",
                        dependent_task_ids=[]
                    ),
                    Task(
                        task_id="2",
                        instruction="Feature engineering",
                        task_type="feature engineering",
                        dependent_task_ids=["1"]
                    ),
                    Task(
                        task_id="3",
                        instruction="Train machine learning model",
                        task_type="machine learning",
                        dependent_task_ids=["2"]
                    ),
                    Task(
                        task_id="4",
                        instruction="Evaluate model performance",
                        task_type="model evaluation",
                        dependent_task_ids=["3"]
                    )
                ]
            }
        ]
        
        for test_case in test_plans:
            print(f"\næµ‹è¯•æ¡ˆä¾‹: {test_case['name']}")
            plan = Plan(goal=test_case['name'])
            plan.add_tasks(test_case['tasks'])
            
            print(f"  ä»»åŠ¡æ•°é‡: {len(plan.tasks)}")
            print(f"  ä»»åŠ¡ç±»å‹: {[task.task_type for task in plan.tasks]}")
            
            start_time = time.time()
            result = adapter.retrieve_workflow_experiences(plan, top_k=3)
            elapsed = time.time() - start_time
            
            print(f"  æ£€ç´¢æ—¶é—´: {elapsed:.3f}ç§’")
            print(f"  æ‰¾åˆ° {len(result.experiences)} ä¸ªç›¸ä¼¼å·¥ä½œæµ")
            
            for i, exp in enumerate(result.experiences, 1):
                print(f"\n  å·¥ä½œæµ {i} (ç›¸ä¼¼åº¦: {exp.score:.3f}):")
                print(f"    ä»»åŠ¡æ•°é‡: {exp.metadata.get('num_tasks', 'N/A')}")
                task_types = exp.metadata.get('task_types', [])
                print(f"    ä»»åŠ¡ç±»å‹: {', '.join(task_types[:5])}{'...' if len(task_types) > 5 else ''}")
                if 'exp' in exp.metadata:
                    exp_preview = exp.metadata['exp'][:100].replace('\n', ' ')
                    print(f"    ç»éªŒæ‘˜è¦: {exp_preview}...")
        
        print(f"\nâœ“ å·¥ä½œæµç»éªŒæ£€ç´¢æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâœ— å·¥ä½œæµç»éªŒæ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_combined_usage():
    """æµ‹è¯•ç»„åˆä½¿ç”¨ï¼ˆæ¨¡æ‹Ÿå®é™…DSAgentæµç¨‹ï¼‰"""
    print("\n\n" + "=" * 80)
    print("æµ‹è¯• 3: ç»„åˆä½¿ç”¨ï¼ˆæ¨¡æ‹Ÿå®é™…DSAgentæµç¨‹ï¼‰")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–å®Œæ•´çš„é€‚é…å™¨
        adapter = MetaGPTAdapter(
            text_exp_path=EXP_PLAN,
            workflow_exp_path=WORKFLOW_EXP
        )
        print(f"âœ“ æˆåŠŸåˆå§‹åŒ–å®Œæ•´é€‚é…å™¨")
        print(f"  æ–‡æœ¬ç»éªŒ: {len(adapter.text_retriever.experiences)} æ¡")
        print(f"  å·¥ä½œæµç»éªŒ: {len(adapter.workflow_retriever.experiences)} ä¸ª")
        
        # æ¨¡æ‹Ÿç”¨æˆ·æŸ¥è¯¢
        user_goal = "Analyze the housing price dataset and build a prediction model"
        print(f"\nç”¨æˆ·ç›®æ ‡: {user_goal}")
        
        # æ­¥éª¤1: æ£€ç´¢ç›¸å…³æ–‡æœ¬ç»éªŒ
        print(f"\næ­¥éª¤ 1: æ£€ç´¢ç›¸å…³æ–‡æœ¬ç»éªŒ...")
        text_result = adapter.retrieve_text_experiences(user_goal, top_k=2)
        print(f"  æ‰¾åˆ° {len(text_result.experiences)} æ¡ç›¸å…³ç»éªŒ")
        for i, exp in enumerate(text_result.experiences, 1):
            print(f"  - ç»éªŒ {i} (å¾—åˆ†: {exp.score:.2f}): {exp.content[:80]}...")
        
        # æ­¥éª¤2: åˆ›å»ºåˆæ­¥è®¡åˆ’
        print(f"\næ­¥éª¤ 2: åˆ›å»ºåˆæ­¥è®¡åˆ’...")
        plan = Plan(goal=user_goal)
        plan.add_tasks([
            Task(task_id="1", instruction="Load housing data", task_type="pda", dependent_task_ids=[]),
            Task(task_id="2", instruction="Exploratory data analysis", task_type="statistical analysis", dependent_task_ids=["1"]),
            Task(task_id="3", instruction="Feature engineering", task_type="feature engineering", dependent_task_ids=["2"]),
            Task(task_id="4", instruction="Build prediction model", task_type="machine learning", dependent_task_ids=["3"]),
            Task(task_id="5", instruction="Evaluate model", task_type="model evaluation", dependent_task_ids=["4"])
        ])
        print(f"  è®¡åˆ’åŒ…å« {len(plan.tasks)} ä¸ªä»»åŠ¡")
        
        # æ­¥éª¤3: æ£€ç´¢ç›¸ä¼¼å·¥ä½œæµ
        print(f"\næ­¥éª¤ 3: æ£€ç´¢ç›¸ä¼¼å·¥ä½œæµ...")
        workflow_result = adapter.retrieve_workflow_experiences(plan, top_k=2)
        print(f"  æ‰¾åˆ° {len(workflow_result.experiences)} ä¸ªç›¸ä¼¼å·¥ä½œæµ")
        for i, exp in enumerate(workflow_result.experiences, 1):
            print(f"  - å·¥ä½œæµ {i} (ç›¸ä¼¼åº¦: {exp.score:.3f}): {exp.metadata.get('num_tasks', 0)} ä¸ªä»»åŠ¡")
        
        # æ­¥éª¤4: æ ¼å¼åŒ–ç”¨äºLLM
        print(f"\næ­¥éª¤ 4: æ ¼å¼åŒ–ç»éªŒç”¨äºLLMæç¤º...")
        text_formatted = adapter.format_experiences_for_prompt(text_result)
        workflow_formatted = adapter.format_experiences_for_prompt(workflow_result)
        
        combined_prompt = f"""
ç”¨æˆ·ç›®æ ‡: {user_goal}

ç›¸å…³æ–‡æœ¬ç»éªŒ:
{text_formatted}

ç›¸ä¼¼å·¥ä½œæµ:
{workflow_formatted}

è¯·åŸºäºä»¥ä¸Šç»éªŒåˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’...
"""
        print(f"  ç»„åˆæç¤ºé•¿åº¦: {len(combined_prompt)} å­—ç¬¦")
        print(f"  æç¤ºé¢„è§ˆï¼ˆå‰300å­—ç¬¦ï¼‰:")
        print(combined_prompt[:300].replace('\n', '\n  '))
        
        print(f"\nâœ“ ç»„åˆä½¿ç”¨æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâœ— ç»„åˆä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_compatibility_with_existing_code():
    """æµ‹è¯•ä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§"""
    print("\n\n" + "=" * 80)
    print("æµ‹è¯• 4: ä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§")
    print("=" * 80)
    
    try:
        # æ£€æŸ¥èƒ½å¦æ­£ç¡®è¯»å–ç°æœ‰æ ¼å¼çš„ç»éªŒæ–‡ä»¶
        adapter = MetaGPTAdapter(
            text_exp_path=EXP_PLAN,
            workflow_exp_path=WORKFLOW_EXP
        )
        
        # æµ‹è¯•1: æ£€æŸ¥æ–‡æœ¬ç»éªŒæ ¼å¼
        print("\næ£€æŸ¥æ–‡æœ¬ç»éªŒæ ¼å¼...")
        if adapter.text_retriever.experiences:
            sample_exp = adapter.text_retriever.experiences[0]
            print(f"  âœ“ ç»éªŒæ¡ç›®æ ¼å¼: {type(sample_exp)}")
            print(f"  âœ“ åŒ…å«å­—æ®µ: content={bool(sample_exp.content)}, metadata={bool(sample_exp.metadata)}")
        
        # æµ‹è¯•2: æ£€æŸ¥å·¥ä½œæµç»éªŒæ ¼å¼
        print("\næ£€æŸ¥å·¥ä½œæµç»éªŒæ ¼å¼...")
        if adapter.workflow_retriever.experiences:
            sample_exp = adapter.workflow_retriever.experiences[0]
            print(f"  âœ“ ç»éªŒæ¡ç›®æ ¼å¼: {type(sample_exp)}")
            print(f"  âœ“ åŒ…å«å­—æ®µ: content={bool(sample_exp.content)}, metadata={bool(sample_exp.metadata)}")
            if 'workflow' in sample_exp.metadata:
                print(f"  âœ“ å·¥ä½œæµç»“æ„æ­£ç¡®")
        
        # æµ‹è¯•3: éªŒè¯ä¸MetaGPT Plançš„è½¬æ¢
        print("\næ£€æŸ¥ä¸MetaGPT Plançš„è½¬æ¢...")
        plan = Plan(goal="Test goal")
        plan.add_tasks([
            Task(task_id="1", instruction="Test task", task_type="test", dependent_task_ids=[])
        ])
        workflow_dict = adapter._plan_to_workflow(plan)
        print(f"  âœ“ Plan â†’ workflow dict è½¬æ¢æˆåŠŸ")
        print(f"  âœ“ è½¬æ¢ååŒ…å« {len(workflow_dict)} ä¸ªä»»åŠ¡")
        
        print(f"\nâœ“ å…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâœ— å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 20 + "DSAgent Core ä¸ MetaGPT é›†æˆæµ‹è¯•" + " " * 24 + "â•‘")
    print("â•š" + "=" * 78 + "â•")
    print()
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("æ–‡æœ¬ç»éªŒæ£€ç´¢", test_text_retrieval()))
    results.append(("å·¥ä½œæµç»éªŒæ£€ç´¢", test_workflow_retrieval()))
    results.append(("ç»„åˆä½¿ç”¨", test_combined_usage()))
    results.append(("å…¼å®¹æ€§æ£€æŸ¥", test_compatibility_with_existing_code()))
    
    # è¾“å‡ºæ€»ç»“
    print("\n\n" + "=" * 80)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    
    for test_name, passed in results:
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    total_tests = len(results)
    passed_tests = sum(1 for _, passed in results if passed)
    
    print(f"\næ€»è®¡: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DSAgent Core ä¸ MetaGPT é›†æˆæ­£å¸¸ï¼")
        return 0
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é—®é¢˜ã€‚")
        return 1


if __name__ == "__main__":
    exit(main())
